1. 배깅
bootstrap aggregating의 약자


주어진 훈련 데이터에서 중복을 허용하여 원 데이터셋과 같은 크기의 데이터셋을 만듦
트리들의 편향은 그대로 유지, 분산은 감소시켜 성능 향상
트리들이 서로 상관화correlated 되어 있지 않다면 트리들의 평균은 노이즈에 강인
  
부트스트랩을 통해 조금씩 다른 훈련 데이터에 대해 훈련된 기초 분류기들을 결합
서로 다른 데이터셋들에 대해 훈련시킴으로써 트리들을 비상관화 시켜주는 과정

2. 부트스트랩
훈련 데이터에서 중복을 허용하여 원 데이터셋과 같은 크기의 데이터셋을 만드는 과정
- 트리들의 편향은 유지하면서 분산은 감소시키기 때문에 포레스트의 성능을 향상
- 트리들이 서로 상관화(correlated)되어 있지 않다면 여러 트리들의 평균은 노이즈에 대해 강인
- 여러개의 트리를 만들 수 있기 때문에 오버피팅이 덜 일어날 수 있음

3. Bagging 훈련과정
- 부트스트랩 방법을 통해 T개의 훈련 데이터셋을 생성 - 부트스트랩
- T개의 기초 분류기(트리)들을 훈련 
- 기초 분류기(트리)들을 하나의 분류기(랜던 포레스트)로 집계(평균 또는 과반수투표 방식 이용) - 집계(aggregating)

4. 앙상블 방법(Ensemble Machine Learning Approach)
- 앙상블은 개선된 분류기를 생성하기 위해 일련의 저성능 분류기를 결합한 복합 모델
- 과반수 투표를 수행하는 개별 분류기 투표 및 최종 예측 레이블이 반환
- 배깅 접근법을 사용하여 분산을 줄이고, 부스팅 접근법을 사용하여 편향을 사용 또는 스태킹 접근법으로 예측 계선

5. 랜덤 포레스트
- 분류 회귀 에 사용되는 앙상블 학습 방법의 일종 (cart)
- 주요파라미터: n_estimators : 트리의 수/ criterion: 가지의 분할의 품질을 측정

6. 회귀 평가방법
 R2스퀘어MAE/MASE/RMSE/RMSLE
- R² 는 값이 클수록 성능이 좋음
- MAE/MASE/RMSE/RMSLE는 값이 작을수록 회귀 성능이 좋은 것. 예측값과 실제값의 차이가 없다
- 분류와 회귀는 scoring 하는 방법이 다르다

7.순서대로 나누어 주어야하는 데이터 : 시계열이 있는경우

8. pd.DataFrame(clf.cvresults)


RandomizedSearchCV
verbose: 로그를 찍는 파라미터이다.
n_inter: 빨리 결과값을 보고 싶다면 기본값인 10에서 그 이하로 줄일 수 있다.
n_job: 사용할 코어의 수 (-1로 지정하는 이유는 다른 사용자의 코어의 수가 다 다르기 때문이다.)
n_iter=5, cv=5 총 25번을 학습한다.
